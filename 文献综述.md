- Kearns和valiant首先提出了strongly learnable 和 weakly learnable 的概念，并且指出在probably approximately correct ,PAC的学习框架中，它们是等价的。因此如果已经有了弱学习算法，那么我们感兴趣的话题将集中于，如何将其转化为强学习算法。Adaboost的提出较好的解决了这个问题，其基本思想是，假设给定了一个函数空间H(Hypothesis)，我们最终的分类器是函数空间中向量的线性组合。我们的目标是找到最优的函数f，他最小化了经验损失。一种解决办法是“前向分布算法”，其基本思想是每次只学习一个参数(然后再加权)。不难证明，如果弱学习器比随机猜测的效果好于一个常数的话，那么通过Adaboost集成之后的强学习器，其训练误差将会随着训练次数的增加而呈现出指数下降，而并不对弱学习器的种类进行约束，这一点非常具有吸引力， 同时也是Ada一词的来源。值得注意的是，adaboost可以看作梯度增强算法的特殊情形。Adaboost（或者梯度增强算法）在实证研究中展现出的极高的精确性，近年的Kaggle比赛中有18个获胜队伍利用的是Adaboost或者其变种，相比之下仅有10只队伍使用神经网络模型。在实际应用中，我们不得不在训练模型的开销以及准确度上进行trade off。基于此，微软提出了LightGBM，其基本思想在于“纵向采样”以及“横向捆绑”，具体而言便是，删除梯度较小的样本，同时将稀疏的特征进行捆绑或者删除。Ke and Meng[2017]指出，LightGBM能够在保持精确度的同时，将训练速度提升至20倍。尽管Adaboost具有上述优势，但Baum and Haussler指出，模型的泛化误差将会随着训练次数的增加而呈现出(至多)线性增长。 尽管上述集成模型经过了多年研究，但我认为仍然存在可以改进的地方。本文从，“函数空间”，“正则项”等多个角度，对已有模型进行进一步改进，并尝试结合Lightgbm的思想对模型进行加速，以期达到更好的预测精度。

  

  - 扩展Hypothesis函数空间，减少模型的设定偏误。例如：引入广义线性模型，不同的kernel生成的Hilbert空间(RKHS)等。

    引入正则项，减轻过拟合问题。 例如：在损失函数中引入备选函数的范数进行惩罚。

    对比梯度下降，泛函式梯度下降算法的优劣，尽管后者对假设函数空间H的要求更高。

    训练方法：全样本训练，随机批量训练。

    将上述改进的集成算法分别应用于分类与回归问题中，并与GLM，SVM，FNN，CNN等模型进行对比。

    模型应用：将模型应用于精算以及量化金融等领域。

    由于希尔伯特再生成核空间，决策树桩空间，广义线性模型函数的空间应用，我打算把模型叫做LightGKTBoost.

    

    

相关：GBT, GBDT, GBRT, GBM, MART, RF，representer theorem

Gradient-based One-side sampling(GOSS), Exclusive feature bundling (EFB)

基于PAC(概率近似正确)的框架, Kearns and Valiant首次指出了强可学习，弱可学习的概念。Schapire在1989年首次提出了一个可执行的多项式时间花费的增强算法。Freund再在此基础上开发了一个更有效的算法。Freund and Schapire在1995年引入了Adaboost算法，正如它的名字一样，adaboost算法对weak learner的类别并不敏感，只需要它具有比随机猜测更好的预测精度即可。在Adaboost的基础上，业界和学术界分别发展了不同种类的模型，前者主要关注模型的效率，代表有xgboost,lightgbm，Catboost后者重点关注模型假设的合理性，也就是着手改进假设空间，正则项等。





Since th