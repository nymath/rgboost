\section{Introduction}
Gradient Boosting Machines (GBMs) are highly effective machine learning algorithms extensively utilized for diverse tasks including regression and classification. They build a robust learner by sequentially amalgamating weak learners, exhibiting remarkable performance across various datasets. Consequently, GBMs have emerged as a highly esteemed model in statistical learning, with the entire process often viewed as a variational method where the functional gradient algorithm strives to optimize the goal function.

The concept of boosting weak learners was first proposed by Freund with Adaboost, followed by Schapire~\cite{freund1996experiments}. Friedman later introduced a gradient boosting-based algorithm~\cite{friedman2001greedy}, and the concept of gradient boosting has since been further developed and expanded. In 2001, Breiman introduced Random Forest, a variant of gradient boosting that combines decision trees and random subspace sampling to enhance the algorithm's performance~\cite{breiman2001random}. Subsequently, Hastie, Tibshirani, and Friedman developed Gradient Boosted Regression Trees (GBRT) in 2006, combining decision trees and gradient boosting to create a powerful and flexible machine learning algorithm for regression problems. In 2015, Chen introduced XGBoost, a widely-used implementation of GBM based on decision trees, employing a novel regularization technique to prevent overfitting~\cite{chen2015xgboost}. XGBoost incorporates advanced features such as parallel processing, distributed computing, and early stopping, and has been widely adopted in both industry and research, winning numerous machine learning competitions.

Microsoft introduced another implementation of GBM, LightGBM, in 2017~\cite{ke2017lightgbm}. LightGBM uses a novel histogram-based approach to accelerate gradient computation and includes advanced features such as categorical feature handling and GPU acceleration. LightGBM has been shown to outperform other popular gradient-boosting algorithms in terms of speed and accuracy. These related works have significantly advanced the field of gradient boosting, making it a popular and powerful machine learning technique.

Traditional boosting algorithms often use decision trees as base learners due to their simplicity. However, there is no restriction on the type of base learners that can be employed. The gradient boosting algorithm can be viewed as an approximated functional gradient descent process, wherein a base learner is chosen to approximate the gradient function queried at $(x_1, \cdots, x_n)$ in the $L^2$ norm.

Many studies have focused on choosing different types of base learners, such as Generalized Linear Models (GLMs), Kernel functions, splines, and others. Some research has also attempted to combine different base learners within the boosting framework. For example, Sigrist~\cite{sigrist2021ktboost} and Hoffmann~\cite{hoffmann2004combining} achieved lower bias and error by combining various base learners.

Nonetheless, limited research has been conducted on modifying the negative gradient vector, a critical element in each iteration. We argue that the negative gradient vector used by Friedman~\cite{friedman2001greedy} is biased and propose a superior alternative by assuming the hypothesis space is a Reproducing Kernel Hilbert Space (RKHS).

To validate our model's effectiveness, we first introduce the concept of Fréchet derivatives, essential in functional analysis. We then examine the differential of functionals in function space, playing a crucial role in mathematical analysis. By providing a clear definition of the gradient using the Riesz representation theorem, we establish a rigorous framework for understanding the concept of the gradient in functional spaces. Next, by employing the chain rule of Fréchet derivatives, we calculate the gradient of the empirical loss function and embed it into the boosting algorithm. Finally, simulated data verifies our model's accuracy, with the revised model exhibiting a faster convergence rate and smaller test error.

The remainder of this paper is organized as follows: Section 2 introduces functional analysis concepts and derives an alternative negative gradient vector. We then discuss the regression range and determine which function our model will approximate. In Section 3, we design the gradient boosting machine and introduce the \texttt{rgboost} package in Python. Section 4 presents a series of experiments on simulated data to validate our model's effectiveness.Lastly, Section 5 concludes the paper and highlights its contributions to the field of gradient boosting.