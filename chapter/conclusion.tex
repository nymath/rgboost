\section{Conclusion}

This paper presents a new gradient boosting algorithm called RGBoost, which enhances the accuracy of the gradient function approximation by modifying the negative gradient at each iteration. We define the gradient function strictly using the Riesz representation theorem and demonstrates that the gradient vector in the conventional gradient boosting algorithm is biased if the hypothesis is a Reproducing Kernel Hilbert Space (RKHS). The updated model performs significantly better than the previous model in simulated data and is subsequently applied in quantitative finance. The paper also includes an overview of related works in gradient boosting and ensemble learning, and describes the self-developed RGBoost module in Python.

\appendix
\newpage

\section{Appendix}

\subsection{Proof of Theorem \ref{thm:gradient-of-LL}}
\begin{proof}
%(theorem \ref{thm:ChainRule})
From the chain rules we have the following
\begin{eqnarray*}
	D_{\mathcal{L},f} &=& \frac{1}{n}\sum_{i=1}^n D_{L \circ E_{x_i},f} \\
	&=& \frac{1}{n}\sum_{i=1}^n D_{L,f(x_i)} \circ D_{E_{x_i},f}. \\
\end{eqnarray*} 
In addition,
\begin{eqnarray*}
	D_{\mathcal{L},f}(h) &=& \frac{1}{n}\sum_{i=1}^n D_{L,f(x_i)} \circ D_{E_{x_i},f} \circ h \\
	&=& \frac{1}{n} \sum_{i=1}^{n} \partial_2 L(y_i,f(x_i)) \inner{K_{x_i}}{h} \\
	&=& \inner{\frac{1}{n} \sum_{i=1}^{n} \partial_2 L(y_i,f(x_i)) K_{x_i}}{h}.
\end{eqnarray*}
Now the Riesz representation theorem suggests that the gradient of $\mathcal{L}$ at $f$ is the first element in the inner product notation.
\end{proof}
\subsection{Proof of Theorem \ref{thm:LS-to-RS}}
\begin{proof}
First suppose that $g \circ f$ is a simple measurable function on $\Omega$, that is 
$$
g \circ f = \sum_{k=1}^{n}c_k \chi_{F_k}, \quad F_k \in \mathcal{F}.
$$
Easy to verify that 
$$
g \circ f \circ f^{-1} = \sum_{k=1}^n c_k \chi_{f(F_k)}, 
$$
Also, we have
\begin{eqnarray*}
	\int_\R g \circ I \D \mu_f &=&  \int_\R \sum_{k=1}^n c_k \chi_{f(F_k)} \D \mu_f \\
	&=&\sum_{k=1}^n c_k \mu_f \circ f(F_k) \\
	&=& \sum_{k=1}^n c_k P \circ f^{-1} \circ f \circ F_k \\
	&=& \sum_{k=1}^n c_k P(F_k) \\
	&=& \int_{\Omega} g \circ f \D P.
\end{eqnarray*}
Since the simple-measuralbe functions are dense in $\mathcal{L}^1(\Omega,\mathcal{F},P)$, we claim that equation \ref{thm:changeofmeasure} holds if $ g \circ f \in \mathcal{L}^1(\Omega,\mathcal{F},P).$
\end{proof}


\subsection{Proof of Theorem \ref{thm:relationship}}
\begin{proof}
From theorem \ref{thm:ExistenceOfFunc} we know that $m \circ X$ is $\mathcal{F}$-measurable. Next we need to verify whether the partial average of $m \circ X$ equals to that of $Y$.
\begin{eqnarray*}
	\int_A m \circ X \D P &=& \int_\Omega \chi_A m \circ X \D P \\
	&=& \int_{\R^p} \chi_{X(A)}(x) m(x) \D \mu_X(x) \\
	&=& \int_{\R^p} \chi_{X(A)}(x) m(x) f_X(x) \D \lambda^p (x) \\
	&=& \int_{\R^p} \chi_{X(A)}(x)f_X(x) \frac{\int_\R yf(x,y) \D \lambda(y)}{f_X(x)}\D \lambda^p(x) \\
	&=& \int_{\R^p}\int_\R \chi_{X(A)}(x) y f(x,y) \D \lambda(y) \D \lambda^p(x) \\
	&=& \int_{\R^{p+1}} \chi_{X(A)}(x) y \D \mu_{X,Y}(x,y) \\
	&=& \int_A Y \D P,
\end{eqnarray*}
where the second and last equaty follows from Theorem \ref{thm:changeofmeasure}.
According to the definition of conditional expectation, we know that $m \circ X$ is the conditional expectation of $Y$ with respect to the sigma algebra generated by $X$.
\end{proof}






\section{Properties of Conditional Expectation}

\begin{Definition}[The conditional Expectation with respect a sigma-algebra] \ \\
Suppose $\MCG$ is sub-sigma-algebra of $\MCF$, $Y$ is random variable. A random variable $M$ is called the conditional expectation if 
\begin{enumerate}
	\item $M$ is $\MCG$-measurable.
	\item $\forall A \in \MCG$, $\int_A X \D P = \int_A M \D P$.  
\end{enumerate}
\end{Definition}

$M$ is often denoted by $E(X|\MCG)$.


\begin{Theorem}[Radon-Nikodym Theorem] \ \\
Suppose $\mu$ is a sigma-finite measure on a measurable space $(X,\mathcal{S})$. Suppose $\nu$ is a sigma-finite on $X, \mathcal{S}$ such that $\nu << \mu$. Then there exists $h \in \mathcal{L}^{1}(\mu)$ such that
$$
\D \nu = h \D \mu.
$$
\end{Theorem}

\begin{Definition}{The conditional Expectation with respect a random map} \ \\
Suppose $X$ is measurable map from $\Omega$ to $\mathcal{E}$(e.g. $\R^p$) and $Y$ is a random variable, then the conditional expecation of $Y$ with respect to $X$ is defined as 
$$
E(Y|X) = E(Y| X^{-1}(\mathcal{E})).
$$
\end{Definition}


\begin{Theorem}\label{thm:ExistenceOfFunc}
Suppose $X: \Omega \to E$, then $Y: \Omega \to \R$ is $\sigma(X)$-measurable if and only if there exists $h: E \to \R$, $h \in \mathcal{E}$, such that 
$$
Y = h \circ X.
$$
\end{Theorem}

%\begin{Example}
%Suppose $E \subset \R^n$, $(\Omega, \MCF, P)$ is a probability space, $X: \Omega \to E$(a random vector).
%\end{Example}


